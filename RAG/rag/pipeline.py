# ============================================================
# pipeline.py
# ============================================================

import faiss
from datetime import datetime
from rag.config  import (MAX_PER_FEED, ALL_CANDIDATE_FEEDS,
    SITEMAP_SOURCES, BACKFILL_START_YEAR,
    BACKFILL_END_YEAR)
from rag.scraper import (scrape_feeds, test_feeds,
    collect_sitemap_urls, scrape_url_batch)
from rag.chunker import chunk_articles
from rag.store   import (init_db, save_to_index, load_seen_urls,
    get_next_chunk_id, storage_report)
from rag.models  import embedding_model


# â”€â”€ Shared embed + save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _embed_and_save(articles: list[dict], label: str = "") -> int:
    """
    Chunk â†’ embed â†’ persist. Returns number of chunks saved.
    INSERT OR IGNORE in store.py means this is always safe to re-call.
    """
    if not articles:
        return 0

    start_id   = get_next_chunk_id()
    new_chunks = chunk_articles(articles, start_id=start_id)

    if not new_chunks:
        return 0

    print(f"  ğŸ”¢ Embedding {len(new_chunks)} chunksâ€¦")
    texts      = [c['text'] for c in new_chunks]
    embeddings = embedding_model.encode(
        texts,
        batch_size=64,
        show_progress_bar=True,
        convert_to_numpy=True,
    )
    faiss.normalize_L2(embeddings)
    save_to_index(new_chunks, embeddings)
    return len(new_chunks)


# â”€â”€ Daily refresh (RSS â€” unchanged call signature) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def daily_refresh(feed_urls:    list[str] | None = None,
                  max_per_feed: int               = MAX_PER_FEED) -> None:
    """
    Incremental RSS update â€” run every Colab session.
    Call signature is identical to before â€” notebook cell unchanged.
    """
    print("=" * 60)
    print(f"ğŸ—“ï¸  Daily Refresh â€” {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 60)

    init_db()

    if feed_urls is None:
        feed_urls = test_feeds(ALL_CANDIDATE_FEEDS)

    seen_urls = load_seen_urls()
    print(f"\nğŸ“¦ {len(seen_urls):,} URLs already indexed â€” will skip.\n")

    new_articles = scrape_feeds(feed_urls,
                                max_per_feed=max_per_feed,
                                skip_urls=seen_urls)

    if not new_articles:
        print("\nâœ… Nothing new to index â€” already up to date.")
        storage_report()
        return

    saved = _embed_and_save(new_articles, label="Daily refresh")
    print(f"\nğŸ‰ Daily refresh done! "
        f"+{len(new_articles)} articles | +{saved} chunks")
    storage_report()


# â”€â”€ Backfill (sitemap â€” fault-tolerant, year-by-year) â”€â”€â”€â”€â”€â”€â”€â”€â”€

def backfill(sitemap_urls:  list[str] | None = None,
             articles_per_batch: int          = 3000,
             batch_size:    int | None        = None,
             start_year:    int | None        = None,
             end_year:      int | None        = None) -> None:
    """
    Fault-tolerant historical backfill via sitemaps.

    Architecture:
      For each year (oldest â†’ newest):
        1. Parse sitemaps to collect ALL article URLs for that year
        2. Filter out already-indexed URLs (loaded fresh from DB each time)
        3. Split remaining URLs into batches of articles_per_batch
        4. For each batch: scrape â†’ embed â†’ save â†’ report â†’ next batch
        5. If Colab disconnects, re-running safely skips already-saved URLs

    Args:
        sitemap_urls:       Override default SITEMAP_SOURCES
        articles_per_batch: Scrape and save this many articles before
                            persisting (default 3000 â€” ~15-20 min per batch)
        start_year:         Override BACKFILL_START_YEAR
        end_year:           Override BACKFILL_END_YEAR
    """
    if batch_size is not None:
        articles_per_batch = batch_size
    print("=" * 60)
    print(f"ğŸ“š Backfill â€” {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 60)

    init_db()

    if sitemap_urls is None:
        sitemap_urls = SITEMAP_SOURCES
    yr_start = start_year or BACKFILL_START_YEAR
    yr_end   = end_year   or BACKFILL_END_YEAR

    print(f"\nğŸ“… Target range: {yr_start} â†’ {yr_end}")
    print(f"ğŸ“¦ Batch size: {articles_per_batch:,} articles per save\n")

    # Show what's already stored before we begin
    storage_report()

    grand_total_articles = 0
    grand_total_chunks   = 0

    for year in range(yr_start, yr_end + 1):
        print("\n" + "â–ˆ" * 60)
        print(f"â–ˆ  YEAR {year}")
        print("â–ˆ" * 60)

        # Load seen URLs fresh at the start of each year
        # so previous year's saves are already excluded
        seen_urls = load_seen_urls()
        print(f"  ğŸ“¦ {len(seen_urls):,} URLs already in DB (will skip)\n")

        # Step 1 â€” collect all URLs for this year
        print(f"  ğŸ—ºï¸  Collecting sitemap URLs for {year}â€¦")
        year_urls = collect_sitemap_urls(
            sitemap_urls,
            target_year=year,
            skip_urls=seen_urls
        )

        if not year_urls:
            print(f"  âœ… No new URLs found for {year} â€” already complete.\n")
            continue

        print(f"\n  ğŸ“‹ {len(year_urls):,} new URLs to scrape for {year}")

        # Step 2 â€” split into batches and process
        batches      = [year_urls[i:i + articles_per_batch]
            for i in range(0, len(year_urls), articles_per_batch)]
        year_articles = 0
        year_chunks   = 0

        for batch_num, batch_urls in enumerate(batches, 1):
            print(f"\n  â”Œâ”€ Batch {batch_num}/{len(batches)} "
                f"({len(batch_urls):,} URLs) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

            # Reload seen_urls before each batch â€” catches mid-year saves
            seen_urls = load_seen_urls()
            batch_urls = [u for u in batch_urls if u not in seen_urls]

            if not batch_urls:
                print(f"  â”‚  All URLs in this batch already indexed â€” skipping.")
                print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
                continue

            print(f"  â”‚  Scraping {len(batch_urls):,} URLsâ€¦")

            # Scrape this batch
            articles = scrape_url_batch(
                batch_urls,
                batch_num=batch_num,
                total_batches=len(batches)
            )

            if not articles:
                print(f"  â”‚  âš ï¸  No articles extracted from this batch.")
                print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
                continue

            # Embed and save immediately â€” before moving to next batch
            print(f"\n  â”‚  ğŸ’¾ Persisting {len(articles):,} articles to Driveâ€¦")
            saved_chunks = _embed_and_save(articles)

            year_articles += len(articles)
            year_chunks   += saved_chunks

            print(f"  â””â”€ âœ… Batch {batch_num} complete: "
                f"+{len(articles):,} articles | +{saved_chunks:,} chunks")

            # Storage report after every batch so you can see growth
            storage_report()

        print(f"\n  ğŸ Year {year} complete: "
            f"+{year_articles:,} articles | +{year_chunks:,} chunks")

        grand_total_articles += year_articles
        grand_total_chunks   += year_chunks

    # Final report
    print("\n" + "=" * 60)
    print("ğŸ‰ BACKFILL COMPLETE")
    print("=" * 60)
    print(f"  Total new articles : {grand_total_articles:,}")
    print(f"  Total new chunks   : {grand_total_chunks:,}")
    storage_report()
